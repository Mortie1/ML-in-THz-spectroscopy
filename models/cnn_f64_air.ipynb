{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1000</th>\n",
       "      <th>element_1</th>\n",
       "      <th>element_2</th>\n",
       "      <th>element_3</th>\n",
       "      <th>element_1_ratio</th>\n",
       "      <th>element_2_ratio</th>\n",
       "      <th>element_3_ratio</th>\n",
       "      <th>temp</th>\n",
       "      <th>pressure</th>\n",
       "      <th>air_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.402026e-21</td>\n",
       "      <td>1.650584e-21</td>\n",
       "      <td>1.910661e-21</td>\n",
       "      <td>2.264304e-21</td>\n",
       "      <td>2.829955e-21</td>\n",
       "      <td>3.673546e-21</td>\n",
       "      <td>4.745899e-21</td>\n",
       "      <td>5.773241e-21</td>\n",
       "      <td>6.446840e-21</td>\n",
       "      <td>6.741727e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466256e-20</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.440157</td>\n",
       "      <td>0.559843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.102767e-22</td>\n",
       "      <td>4.371429e-22</td>\n",
       "      <td>4.698476e-22</td>\n",
       "      <td>5.105580e-22</td>\n",
       "      <td>5.626400e-22</td>\n",
       "      <td>6.315813e-22</td>\n",
       "      <td>7.266133e-22</td>\n",
       "      <td>8.610945e-22</td>\n",
       "      <td>1.048941e-21</td>\n",
       "      <td>1.323529e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>2.481710e-21</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.428047</td>\n",
       "      <td>0.571953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.610634e-23</td>\n",
       "      <td>2.739156e-23</td>\n",
       "      <td>1.628379e-23</td>\n",
       "      <td>1.195799e-23</td>\n",
       "      <td>1.069667e-23</td>\n",
       "      <td>1.058083e-23</td>\n",
       "      <td>1.121691e-23</td>\n",
       "      <td>1.274356e-23</td>\n",
       "      <td>1.515703e-23</td>\n",
       "      <td>1.882304e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>2.756785e-21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0.342547</td>\n",
       "      <td>0.376666</td>\n",
       "      <td>0.280788</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.016487e-22</td>\n",
       "      <td>1.356304e-22</td>\n",
       "      <td>1.894228e-22</td>\n",
       "      <td>3.406116e-22</td>\n",
       "      <td>8.410447e-22</td>\n",
       "      <td>2.039525e-21</td>\n",
       "      <td>1.635235e-21</td>\n",
       "      <td>6.587985e-22</td>\n",
       "      <td>3.087047e-22</td>\n",
       "      <td>2.124933e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240249e-22</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.321178</td>\n",
       "      <td>0.625524</td>\n",
       "      <td>293.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.637227e-21</td>\n",
       "      <td>2.113385e-21</td>\n",
       "      <td>2.633061e-21</td>\n",
       "      <td>3.098701e-21</td>\n",
       "      <td>3.509187e-21</td>\n",
       "      <td>3.569827e-21</td>\n",
       "      <td>3.557713e-21</td>\n",
       "      <td>3.356628e-21</td>\n",
       "      <td>3.389955e-21</td>\n",
       "      <td>3.744591e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>3.614986e-23</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>0.470739</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.277796</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183548</th>\n",
       "      <td>1.720379e-22</td>\n",
       "      <td>2.933964e-22</td>\n",
       "      <td>4.190668e-22</td>\n",
       "      <td>5.128756e-22</td>\n",
       "      <td>6.537258e-22</td>\n",
       "      <td>6.598291e-22</td>\n",
       "      <td>7.231058e-22</td>\n",
       "      <td>6.818061e-22</td>\n",
       "      <td>8.002978e-22</td>\n",
       "      <td>1.130003e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>8.585612e-23</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>0.401527</td>\n",
       "      <td>0.454117</td>\n",
       "      <td>0.144356</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183549</th>\n",
       "      <td>2.392508e-21</td>\n",
       "      <td>1.998411e-21</td>\n",
       "      <td>1.320957e-21</td>\n",
       "      <td>9.131813e-22</td>\n",
       "      <td>6.948151e-22</td>\n",
       "      <td>5.720024e-22</td>\n",
       "      <td>4.987314e-22</td>\n",
       "      <td>4.533583e-22</td>\n",
       "      <td>4.270046e-22</td>\n",
       "      <td>4.143369e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.460996e-21</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.348675</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.261341</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183550</th>\n",
       "      <td>1.947286e-22</td>\n",
       "      <td>2.092204e-22</td>\n",
       "      <td>2.261748e-22</td>\n",
       "      <td>2.462494e-22</td>\n",
       "      <td>2.703056e-22</td>\n",
       "      <td>2.995569e-22</td>\n",
       "      <td>3.357735e-22</td>\n",
       "      <td>3.814871e-22</td>\n",
       "      <td>4.405433e-22</td>\n",
       "      <td>5.189812e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.838447e-23</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>0.430989</td>\n",
       "      <td>0.452610</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183551</th>\n",
       "      <td>4.523992e-22</td>\n",
       "      <td>6.372981e-22</td>\n",
       "      <td>1.661855e-21</td>\n",
       "      <td>1.874233e-22</td>\n",
       "      <td>1.226359e-22</td>\n",
       "      <td>1.804612e-22</td>\n",
       "      <td>3.107570e-22</td>\n",
       "      <td>1.559673e-21</td>\n",
       "      <td>3.250324e-21</td>\n",
       "      <td>2.427107e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508687e-21</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.441890</td>\n",
       "      <td>0.135979</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183552</th>\n",
       "      <td>4.111637e-21</td>\n",
       "      <td>9.719901e-21</td>\n",
       "      <td>4.291975e-21</td>\n",
       "      <td>1.383255e-22</td>\n",
       "      <td>9.272536e-23</td>\n",
       "      <td>5.006519e-22</td>\n",
       "      <td>7.290239e-22</td>\n",
       "      <td>1.258661e-21</td>\n",
       "      <td>2.270487e-21</td>\n",
       "      <td>4.044166e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.719644e-21</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0.425460</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.537220</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183553 rows × 1010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1             2             3             4  \\\n",
       "0       1.402026e-21  1.650584e-21  1.910661e-21  2.264304e-21  2.829955e-21   \n",
       "1       4.102767e-22  4.371429e-22  4.698476e-22  5.105580e-22  5.626400e-22   \n",
       "2       3.610634e-23  2.739156e-23  1.628379e-23  1.195799e-23  1.069667e-23   \n",
       "3       1.016487e-22  1.356304e-22  1.894228e-22  3.406116e-22  8.410447e-22   \n",
       "4       1.637227e-21  2.113385e-21  2.633061e-21  3.098701e-21  3.509187e-21   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "183548  1.720379e-22  2.933964e-22  4.190668e-22  5.128756e-22  6.537258e-22   \n",
       "183549  2.392508e-21  1.998411e-21  1.320957e-21  9.131813e-22  6.948151e-22   \n",
       "183550  1.947286e-22  2.092204e-22  2.261748e-22  2.462494e-22  2.703056e-22   \n",
       "183551  4.523992e-22  6.372981e-22  1.661855e-21  1.874233e-22  1.226359e-22   \n",
       "183552  4.111637e-21  9.719901e-21  4.291975e-21  1.383255e-22  9.272536e-23   \n",
       "\n",
       "                   5             6             7             8             9  \\\n",
       "0       3.673546e-21  4.745899e-21  5.773241e-21  6.446840e-21  6.741727e-21   \n",
       "1       6.315813e-22  7.266133e-22  8.610945e-22  1.048941e-21  1.323529e-21   \n",
       "2       1.058083e-23  1.121691e-23  1.274356e-23  1.515703e-23  1.882304e-23   \n",
       "3       2.039525e-21  1.635235e-21  6.587985e-22  3.087047e-22  2.124933e-22   \n",
       "4       3.569827e-21  3.557713e-21  3.356628e-21  3.389955e-21  3.744591e-21   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "183548  6.598291e-22  7.231058e-22  6.818061e-22  8.002978e-22  1.130003e-21   \n",
       "183549  5.720024e-22  4.987314e-22  4.533583e-22  4.270046e-22  4.143369e-22   \n",
       "183550  2.995569e-22  3.357735e-22  3.814871e-22  4.405433e-22  5.189812e-22   \n",
       "183551  1.804612e-22  3.107570e-22  1.559673e-21  3.250324e-21  2.427107e-21   \n",
       "183552  5.006519e-22  7.290239e-22  1.258661e-21  2.270487e-21  4.044166e-21   \n",
       "\n",
       "        ...          1000  element_1  element_2  element_3  element_1_ratio  \\\n",
       "0       ...  3.466256e-20         17         23         -1         0.440157   \n",
       "1       ...  2.481710e-21          0         19         -1         0.428047   \n",
       "2       ...  2.756785e-21          2         11         13         0.342547   \n",
       "3       ...  1.240249e-22          6         10         13         0.053299   \n",
       "4       ...  3.614986e-23          9         18         24         0.470739   \n",
       "...     ...           ...        ...        ...        ...              ...   \n",
       "183548  ...  8.585612e-23          9         17         23         0.401527   \n",
       "183549  ...  2.460996e-21          4          8         16         0.348675   \n",
       "183550  ...  1.838447e-23          1          7         14         0.116401   \n",
       "183551  ...  1.508687e-21         15         18         23         0.422131   \n",
       "183552  ...  1.719644e-21          3         12         23         0.425460   \n",
       "\n",
       "        element_2_ratio  element_3_ratio   temp  pressure  air_ratio  \n",
       "0              0.559843         0.000000  283.0     0.775        0.0  \n",
       "1              0.571953         0.000000  303.0     0.800        0.6  \n",
       "2              0.376666         0.280788  283.0     0.500        0.6  \n",
       "3              0.321178         0.625524  293.0     0.900        0.6  \n",
       "4              0.251465         0.277796  273.0     0.500        0.6  \n",
       "...                 ...              ...    ...       ...        ...  \n",
       "183548         0.454117         0.144356  313.0     0.500        0.6  \n",
       "183549         0.389984         0.261341  323.0     1.000        0.6  \n",
       "183550         0.430989         0.452610  313.0     0.100        0.6  \n",
       "183551         0.441890         0.135979  283.0     0.200        0.3  \n",
       "183552         0.037319         0.537220  273.0     0.100        0.6  \n",
       "\n",
       "[183553 rows x 1010 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.concat([pd.read_csv(\"more-elements/more-elements.csv\"), pd.read_csv(\"new-more-data/new-more-data.csv\")])\n",
    "# df\n",
    "df = pd.read_pickle(\"half_data.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elems = df[\"element_1\"].unique()\n",
    "# elem2id = {}\n",
    "# for i, elem in enumerate(elems):\n",
    "#     elem2id[elem] = i\n",
    "\n",
    "# df.fillna(\n",
    "#     {\n",
    "#         \"element_1\": \"UNKNOWN\",\n",
    "#         \"element_2\": \"UNKNOWN\",\n",
    "#         \"element_3\": \"UNKNOWN\",\n",
    "#         \"element_1_ratio\": 0.0,\n",
    "#         \"element_2_ratio\": 0.0,\n",
    "#         \"element_3_ratio\": 0.0,\n",
    "#     },\n",
    "#     inplace=True,\n",
    "# )\n",
    "\n",
    "# df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# elem2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"element_1\"] = df[\"element_1\"].apply(\n",
    "#     lambda x: elem2id[x] if x != \"UNKNOWN\" else -1\n",
    "# )\n",
    "# df[\"element_2\"] = df[\"element_2\"].apply(lambda x: elem2id[x] if x != \"UNKNOWN\" else -1)\n",
    "# df[\"element_3\"] = df[\"element_3\"].apply(lambda x: elem2id[x] if x != \"UNKNOWN\" else -1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183553\n",
      "183553\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[(df[[str(i) for i in range(1001)]] > 0).all(axis=1)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11451733286843582"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df[\"element_1\"] == 0) | (df[\"element_2\"] == 0) | (df[\"element_3\"] == 0)]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectraDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, device=\"cuda:0\") -> None:\n",
    "        self.data = data\n",
    "        self.elements = self.data[\"element_1\"].unique()\n",
    "        self.air_ratios = self.data.air_ratio.to_numpy(dtype=np.float64)\n",
    "\n",
    "        self.spectras = torch.log(\n",
    "            torch.tensor(df[[str(i) for i in range(1001)]].to_numpy(dtype=np.float64))\n",
    "        ).to(device)\n",
    "\n",
    "        self.ratios = torch.tensor(\n",
    "            df[[\"element_1_ratio\", \"element_2_ratio\", \"element_3_ratio\"]].to_numpy(\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        ).to(device)\n",
    "\n",
    "        self.element_indices = df[[\"element_1\", \"element_2\", \"element_3\"]].to_numpy(\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.elements_distributions = torch.zeros(\n",
    "            [len(self.data), len(self.elements) + 1], dtype=torch.float64\n",
    "        ).to(device)\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            indices = self.element_indices[idx, :]\n",
    "            indices = indices[indices != -1]\n",
    "            self.elements_distributions[idx, indices] = self.ratios[idx][\n",
    "                range(indices.shape[0])\n",
    "            ] * (1 - self.air_ratios[idx])\n",
    "            self.elements_distributions[idx, -1] = self.air_ratios[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        spectra = self.spectras[idx]\n",
    "        elements_distribution = self.elements_distributions[idx]\n",
    "\n",
    "        return spectra, elements_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzResNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "        )\n",
    "        self.skip_connection = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                in_channels // 4,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                in_channels // 4,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "        self.skip_connection = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1\n",
    "        ) if in_channels != out_channels else lambda x: x\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzCNN(nn.Module):\n",
    "    def __init__(self, n_elements: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_elements = n_elements\n",
    "        self.net = nn.Sequential(\n",
    "            # input_shape: (batch_size, 1, 1001)\n",
    "            nn.BatchNorm1d(1),\n",
    "            THzResNetBlock(in_channels=1, out_channels=64, kernel_size=7),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            # input_shape: (batch_size, 64, 250)\n",
    "            THzResNetBlock(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 128, 50)\n",
    "            THzResNetBlock(in_channels=128, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 256, 10)\n",
    "            THzResNetBlock(in_channels=256, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 512, 2)\n",
    "            nn.Flatten(),\n",
    "            # linear head\n",
    "            nn.Linear(512 * 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.n_elements),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb.wandb_run\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    scheduler: LRScheduler = None,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    model: nn.Module, dataloader: DataLoader, loss_fn: nn.Module\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    mae = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Validating\"):\n",
    "\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss += float(loss_fn(pred, target).item())\n",
    "        mae += float(nn.L1Loss()(nn.Softmax()(pred), target).item())\n",
    "\n",
    "    return loss / len(dataloader), mae / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    n_epochs: int,\n",
    "    loss_fn: nn.Module = nn.CrossEntropyLoss(reduction=\"mean\"),\n",
    "    scheduler: LRScheduler = None,\n",
    "    run: wandb.wandb_run.Run = None\n",
    ") -> None:\n",
    "\n",
    "    for i in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, scheduler\n",
    "        )\n",
    "\n",
    "        val_loss, mae = val_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i}: \\n   Train loss: {train_loss:.5f}    |   Val loss: {val_loss:.5f}   |   Val MAE: {mae:.5f}\\n\"\n",
    "        )\n",
    "        if run:\n",
    "            run.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"MAE\": mae})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "996     False\n",
       "997     False\n",
       "998     False\n",
       "999     False\n",
       "1000    False\n",
       "Length: 1001, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df[[str(i) for i in range(1001)]] < 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "n_epochs = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomSpectraDataset(train_df)\n",
    "val_dataset = CustomSpectraDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1fc49446f20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iyditl5g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc5270f86374dc1b29bffa31307fb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Add 2 more layers + less dropout</strong> at: <a href='https://wandb.ai/max23-ost/course-work/runs/iyditl5g' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/iyditl5g</a><br/> View project at: <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_024201-iyditl5g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iyditl5g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9217a26f05554d23a4eb972d127f9601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\whoee\\Desktop\\CourseWork\\models\\wandb\\run-20240428_024503-1aja3ojj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/max23-ost/course-work/runs/1aja3ojj' target=\"_blank\">Less dropout</a></strong> to <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/max23-ost/course-work/runs/1aja3ojj' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/1aja3ojj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"course-work\",\n",
    "    save_code=True,\n",
    "    group=\"CNN\",\n",
    "    name=\"Less dropout\",\n",
    "    notes=\"Model stopped overfitting. Use a little bit less dropout and log mae metric\",\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = []\n",
    "# for i in range(50000):\n",
    "#     scheduler.step()\n",
    "#     lrs.append(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "THzCNN(\n",
       "  (net): Sequential(\n",
       "    (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (10): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (11): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (14): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (15): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (16): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Flatten(start_dim=1, end_dim=-1)\n",
       "    (18): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (21): ReLU()\n",
       "    (22): Linear(in_features=1024, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = THzCNN(val_dataset[0][1].shape[0])\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "# optimizer = SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler1 = LinearLR(optimizer, start_factor=0.05, end_factor=1, total_iters=500)\n",
    "# scheduler2 = CosineAnnealingWarmRestarts(optimizer, T_0=1500, T_mult=2)\n",
    "# scheduler = SequentialLR(\n",
    "#     optimizer, schedulers=[scheduler1, scheduler2], milestones=[500]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:48<00:00,  3.15it/s]\n",
      "Validating:   0%|          | 0/861 [00:00<?, ?it/s]c:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "   Train loss: 1.56225    |   Val loss: 1.41226   |   Val MAE: 0.02424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:33<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: \n",
      "   Train loss: 1.38867    |   Val loss: 1.36462   |   Val MAE: 0.02303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: \n",
      "   Train loss: 1.35560    |   Val loss: 1.34415   |   Val MAE: 0.02261\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:30<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: \n",
      "   Train loss: 1.33668    |   Val loss: 1.33050   |   Val MAE: 0.02236\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:39<00:00,  3.17it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: \n",
      "   Train loss: 1.32376    |   Val loss: 1.32064   |   Val MAE: 0.02199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: \n",
      "   Train loss: 1.31415    |   Val loss: 1.31580   |   Val MAE: 0.02187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:30<00:00,  3.19it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: \n",
      "   Train loss: 1.30631    |   Val loss: 1.31245   |   Val MAE: 0.02174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: \n",
      "   Train loss: 1.30012    |   Val loss: 1.30797   |   Val MAE: 0.02160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:33<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: \n",
      "   Train loss: 1.29441    |   Val loss: 1.30589   |   Val MAE: 0.02152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: \n",
      "   Train loss: 1.28917    |   Val loss: 1.30402   |   Val MAE: 0.02145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: \n",
      "   Train loss: 1.28407    |   Val loss: 1.30460   |   Val MAE: 0.02140\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: \n",
      "   Train loss: 1.27823    |   Val loss: 1.30510   |   Val MAE: 0.02143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:32<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: \n",
      "   Train loss: 1.27203    |   Val loss: 1.30563   |   Val MAE: 0.02147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:30<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: \n",
      "   Train loss: 1.26560    |   Val loss: 1.30717   |   Val MAE: 0.02147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:30<00:00,  3.19it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: \n",
      "   Train loss: 1.25857    |   Val loss: 1.31217   |   Val MAE: 0.02171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:29<00:00,  3.19it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: \n",
      "   Train loss: 1.25073    |   Val loss: 1.31658   |   Val MAE: 0.02177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:31<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: \n",
      "   Train loss: 1.24262    |   Val loss: 1.32256   |   Val MAE: 0.02190\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:30<00:00,  3.19it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: \n",
      "   Train loss: 1.23442    |   Val loss: 1.32946   |   Val MAE: 0.02200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 4826/4876 [25:17<00:15,  3.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=None)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, n_epochs, loss_fn, scheduler, run)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m      2\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m      3\u001b[0m     train_loader: DataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     run: wandb\u001b[38;5;241m.\u001b[39mwandb_run\u001b[38;5;241m.\u001b[39mRun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         val_loss, mae \u001b[38;5;241m=\u001b[39m val_epoch(model, val_loader, loss_fn)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m    |   Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   |   Val MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[30], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, loss_fn, scheduler)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectra, target \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectra\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, target)\n\u001b[0;32m     17\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 42\u001b[0m, in \u001b[0;36mTHzCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m, in \u001b[0;36mTHzResNetBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 44\u001b[0m     unprocessed_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connection(x)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing(unprocessed_result)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=run)\n",
    "# train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for spectra, target in tqdm(train_loader, desc=\"Training\"):\n",
    "#         optimizer.zero_grad()\n",
    "#         pred = net(spectra[:, None, :])\n",
    "#         loss = nn.CrossEntropyLoss()(pred, target)\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         i += 1\n",
    "#         if i >= 10:\n",
    "#             break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfcc56083f54ae5a941551c5886ea6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▂▂▂▂</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>0.022</td></tr><tr><td>train_loss</td><td>1.23442</td></tr><tr><td>val_loss</td><td>1.32946</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Less dropout</strong> at: <a href='https://wandb.ai/max23-ost/course-work/runs/1aja3ojj' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/1aja3ojj</a><br/> View project at: <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_024503-1aja3ojj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./air-dropout.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on MSE\n",
    "import wandb.wandb_run\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    scheduler: LRScheduler = None,\n",
    ") -> float:\n",
    "    total_loss = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss = nn.MSELoss()(nn.Softmax()(pred), target)\n",
    "\n",
    "        total_loss += loss_fn(pred, target).item()\n",
    "\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(model: nn.Module, dataloader: DataLoader, loss_fn: nn.Module) -> float:\n",
    "    loss = 0\n",
    "    mae = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Validating\"):\n",
    "\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss += float(loss_fn(pred, target).item())\n",
    "        mae += float(nn.L1Loss()(nn.Softmax()(pred), target).item())\n",
    "\n",
    "    return loss / len(dataloader), mae / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\whoee\\Desktop\\CourseWork\\models\\wandb\\run-20240428_112011-zjkitpxq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/max23-ost/course-work/runs/zjkitpxq' target=\"_blank\">MSE v2</a></strong> to <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/max23-ost/course-work/runs/zjkitpxq' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/zjkitpxq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"course-work\",\n",
    "    save_code=True,\n",
    "    group=\"CNN\",\n",
    "    name=\"MSE v2\",\n",
    "    notes=\"Train on MSE + log MAE metric\",\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:52<00:00,  3.14it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "   Train loss: 1.79338    |   Val loss: 1.57833   |   Val MAE: 0.02863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:09<00:00,  3.11it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:59<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: \n",
      "   Train loss: 1.50944    |   Val loss: 1.46837   |   Val MAE: 0.02581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:25<00:00,  3.08it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:58<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: \n",
      "   Train loss: 1.44394    |   Val loss: 1.42879   |   Val MAE: 0.02482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:15<00:00,  3.09it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:58<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: \n",
      "   Train loss: 1.41410    |   Val loss: 1.40488   |   Val MAE: 0.02405\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:47<00:00,  3.15it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: \n",
      "   Train loss: 1.39566    |   Val loss: 1.39226   |   Val MAE: 0.02372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:36<00:00,  3.17it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: \n",
      "   Train loss: 1.38318    |   Val loss: 1.38243   |   Val MAE: 0.02340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [28:07<00:00,  2.89it/s]\n",
      "Validating: 100%|██████████| 861/861 [01:02<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: \n",
      "   Train loss: 1.37231    |   Val loss: 1.37534   |   Val MAE: 0.02309\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:28<00:00,  3.07it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: \n",
      "   Train loss: 1.36393    |   Val loss: 1.36913   |   Val MAE: 0.02289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [25:34<00:00,  3.18it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:57<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: \n",
      "   Train loss: 1.35689    |   Val loss: 1.36779   |   Val MAE: 0.02285\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:21<00:00,  3.08it/s]\n",
      "Validating: 100%|██████████| 861/861 [00:58<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: \n",
      "   Train loss: 1.35086    |   Val loss: 1.36633   |   Val MAE: 0.02284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [26:23<00:00,  3.08it/s]\n",
      "Validating: 100%|██████████| 861/861 [01:00<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: \n",
      "   Train loss: 1.34573    |   Val loss: 1.36756   |   Val MAE: 0.02274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4876/4876 [27:30<00:00,  2.95it/s]\n",
      "Validating: 100%|██████████| 861/861 [01:00<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: \n",
      "   Train loss: 1.34045    |   Val loss: 1.36761   |   Val MAE: 0.02289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 2304/4876 [12:47<14:16,  3.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, n_epochs, loss_fn, scheduler, run)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m      2\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m      3\u001b[0m     train_loader: DataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     run: wandb\u001b[38;5;241m.\u001b[39mwandb_run\u001b[38;5;241m.\u001b[39mRun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         val_loss, mae \u001b[38;5;241m=\u001b[39m val_epoch(model, val_loader, loss_fn)\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m    |   Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   |   Val MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[46], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, loss_fn, scheduler)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(nn\u001b[38;5;241m.\u001b[39mSoftmax()(pred), target)\n\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, target)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# nn.utils.clip_grad_norm_(model.parameters(), 10)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\whoee\\Desktop\\CourseWork\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = THzCNN(val_dataset[0][1].shape[0])\n",
    "net.to(device)\n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "\n",
    "print(device)\n",
    "train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bee52eb5c04ba3baf4f9dcb2ffe1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>█▅▃▃▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>0.02289</td></tr><tr><td>train_loss</td><td>1.34045</td></tr><tr><td>val_loss</td><td>1.36761</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MSE v2</strong> at: <a href='https://wandb.ai/max23-ost/course-work/runs/zjkitpxq' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/zjkitpxq</a><br/> View project at: <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_112011-zjkitpxq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./air-dropout-mse.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
