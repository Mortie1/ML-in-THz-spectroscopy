{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1000</th>\n",
       "      <th>element_1</th>\n",
       "      <th>element_2</th>\n",
       "      <th>element_3</th>\n",
       "      <th>element_1_ratio</th>\n",
       "      <th>element_2_ratio</th>\n",
       "      <th>element_3_ratio</th>\n",
       "      <th>temp</th>\n",
       "      <th>pressure</th>\n",
       "      <th>air_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.402026e-21</td>\n",
       "      <td>1.650584e-21</td>\n",
       "      <td>1.910661e-21</td>\n",
       "      <td>2.264304e-21</td>\n",
       "      <td>2.829955e-21</td>\n",
       "      <td>3.673546e-21</td>\n",
       "      <td>4.745899e-21</td>\n",
       "      <td>5.773241e-21</td>\n",
       "      <td>6.446840e-21</td>\n",
       "      <td>6.741727e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466256e-20</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.440157</td>\n",
       "      <td>0.559843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.102767e-22</td>\n",
       "      <td>4.371429e-22</td>\n",
       "      <td>4.698476e-22</td>\n",
       "      <td>5.105580e-22</td>\n",
       "      <td>5.626400e-22</td>\n",
       "      <td>6.315813e-22</td>\n",
       "      <td>7.266133e-22</td>\n",
       "      <td>8.610945e-22</td>\n",
       "      <td>1.048941e-21</td>\n",
       "      <td>1.323529e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>2.481710e-21</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.428047</td>\n",
       "      <td>0.571953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.610634e-23</td>\n",
       "      <td>2.739156e-23</td>\n",
       "      <td>1.628379e-23</td>\n",
       "      <td>1.195799e-23</td>\n",
       "      <td>1.069667e-23</td>\n",
       "      <td>1.058083e-23</td>\n",
       "      <td>1.121691e-23</td>\n",
       "      <td>1.274356e-23</td>\n",
       "      <td>1.515703e-23</td>\n",
       "      <td>1.882304e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>2.756785e-21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0.342547</td>\n",
       "      <td>0.376666</td>\n",
       "      <td>0.280788</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.016487e-22</td>\n",
       "      <td>1.356304e-22</td>\n",
       "      <td>1.894228e-22</td>\n",
       "      <td>3.406116e-22</td>\n",
       "      <td>8.410447e-22</td>\n",
       "      <td>2.039525e-21</td>\n",
       "      <td>1.635235e-21</td>\n",
       "      <td>6.587985e-22</td>\n",
       "      <td>3.087047e-22</td>\n",
       "      <td>2.124933e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240249e-22</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.053299</td>\n",
       "      <td>0.321178</td>\n",
       "      <td>0.625524</td>\n",
       "      <td>293.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.637227e-21</td>\n",
       "      <td>2.113385e-21</td>\n",
       "      <td>2.633061e-21</td>\n",
       "      <td>3.098701e-21</td>\n",
       "      <td>3.509187e-21</td>\n",
       "      <td>3.569827e-21</td>\n",
       "      <td>3.557713e-21</td>\n",
       "      <td>3.356628e-21</td>\n",
       "      <td>3.389955e-21</td>\n",
       "      <td>3.744591e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>3.614986e-23</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>0.470739</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.277796</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183548</th>\n",
       "      <td>1.720379e-22</td>\n",
       "      <td>2.933964e-22</td>\n",
       "      <td>4.190668e-22</td>\n",
       "      <td>5.128756e-22</td>\n",
       "      <td>6.537258e-22</td>\n",
       "      <td>6.598291e-22</td>\n",
       "      <td>7.231058e-22</td>\n",
       "      <td>6.818061e-22</td>\n",
       "      <td>8.002978e-22</td>\n",
       "      <td>1.130003e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>8.585612e-23</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>0.401527</td>\n",
       "      <td>0.454117</td>\n",
       "      <td>0.144356</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183549</th>\n",
       "      <td>2.392508e-21</td>\n",
       "      <td>1.998411e-21</td>\n",
       "      <td>1.320957e-21</td>\n",
       "      <td>9.131813e-22</td>\n",
       "      <td>6.948151e-22</td>\n",
       "      <td>5.720024e-22</td>\n",
       "      <td>4.987314e-22</td>\n",
       "      <td>4.533583e-22</td>\n",
       "      <td>4.270046e-22</td>\n",
       "      <td>4.143369e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.460996e-21</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.348675</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.261341</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183550</th>\n",
       "      <td>1.947286e-22</td>\n",
       "      <td>2.092204e-22</td>\n",
       "      <td>2.261748e-22</td>\n",
       "      <td>2.462494e-22</td>\n",
       "      <td>2.703056e-22</td>\n",
       "      <td>2.995569e-22</td>\n",
       "      <td>3.357735e-22</td>\n",
       "      <td>3.814871e-22</td>\n",
       "      <td>4.405433e-22</td>\n",
       "      <td>5.189812e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.838447e-23</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>0.430989</td>\n",
       "      <td>0.452610</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183551</th>\n",
       "      <td>4.523992e-22</td>\n",
       "      <td>6.372981e-22</td>\n",
       "      <td>1.661855e-21</td>\n",
       "      <td>1.874233e-22</td>\n",
       "      <td>1.226359e-22</td>\n",
       "      <td>1.804612e-22</td>\n",
       "      <td>3.107570e-22</td>\n",
       "      <td>1.559673e-21</td>\n",
       "      <td>3.250324e-21</td>\n",
       "      <td>2.427107e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508687e-21</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.441890</td>\n",
       "      <td>0.135979</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183552</th>\n",
       "      <td>4.111637e-21</td>\n",
       "      <td>9.719901e-21</td>\n",
       "      <td>4.291975e-21</td>\n",
       "      <td>1.383255e-22</td>\n",
       "      <td>9.272536e-23</td>\n",
       "      <td>5.006519e-22</td>\n",
       "      <td>7.290239e-22</td>\n",
       "      <td>1.258661e-21</td>\n",
       "      <td>2.270487e-21</td>\n",
       "      <td>4.044166e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.719644e-21</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0.425460</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.537220</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183553 rows × 1010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1             2             3             4  \\\n",
       "0       1.402026e-21  1.650584e-21  1.910661e-21  2.264304e-21  2.829955e-21   \n",
       "1       4.102767e-22  4.371429e-22  4.698476e-22  5.105580e-22  5.626400e-22   \n",
       "2       3.610634e-23  2.739156e-23  1.628379e-23  1.195799e-23  1.069667e-23   \n",
       "3       1.016487e-22  1.356304e-22  1.894228e-22  3.406116e-22  8.410447e-22   \n",
       "4       1.637227e-21  2.113385e-21  2.633061e-21  3.098701e-21  3.509187e-21   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "183548  1.720379e-22  2.933964e-22  4.190668e-22  5.128756e-22  6.537258e-22   \n",
       "183549  2.392508e-21  1.998411e-21  1.320957e-21  9.131813e-22  6.948151e-22   \n",
       "183550  1.947286e-22  2.092204e-22  2.261748e-22  2.462494e-22  2.703056e-22   \n",
       "183551  4.523992e-22  6.372981e-22  1.661855e-21  1.874233e-22  1.226359e-22   \n",
       "183552  4.111637e-21  9.719901e-21  4.291975e-21  1.383255e-22  9.272536e-23   \n",
       "\n",
       "                   5             6             7             8             9  \\\n",
       "0       3.673546e-21  4.745899e-21  5.773241e-21  6.446840e-21  6.741727e-21   \n",
       "1       6.315813e-22  7.266133e-22  8.610945e-22  1.048941e-21  1.323529e-21   \n",
       "2       1.058083e-23  1.121691e-23  1.274356e-23  1.515703e-23  1.882304e-23   \n",
       "3       2.039525e-21  1.635235e-21  6.587985e-22  3.087047e-22  2.124933e-22   \n",
       "4       3.569827e-21  3.557713e-21  3.356628e-21  3.389955e-21  3.744591e-21   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "183548  6.598291e-22  7.231058e-22  6.818061e-22  8.002978e-22  1.130003e-21   \n",
       "183549  5.720024e-22  4.987314e-22  4.533583e-22  4.270046e-22  4.143369e-22   \n",
       "183550  2.995569e-22  3.357735e-22  3.814871e-22  4.405433e-22  5.189812e-22   \n",
       "183551  1.804612e-22  3.107570e-22  1.559673e-21  3.250324e-21  2.427107e-21   \n",
       "183552  5.006519e-22  7.290239e-22  1.258661e-21  2.270487e-21  4.044166e-21   \n",
       "\n",
       "        ...          1000  element_1  element_2  element_3  element_1_ratio  \\\n",
       "0       ...  3.466256e-20         17         23         -1         0.440157   \n",
       "1       ...  2.481710e-21          0         19         -1         0.428047   \n",
       "2       ...  2.756785e-21          2         11         13         0.342547   \n",
       "3       ...  1.240249e-22          6         10         13         0.053299   \n",
       "4       ...  3.614986e-23          9         18         24         0.470739   \n",
       "...     ...           ...        ...        ...        ...              ...   \n",
       "183548  ...  8.585612e-23          9         17         23         0.401527   \n",
       "183549  ...  2.460996e-21          4          8         16         0.348675   \n",
       "183550  ...  1.838447e-23          1          7         14         0.116401   \n",
       "183551  ...  1.508687e-21         15         18         23         0.422131   \n",
       "183552  ...  1.719644e-21          3         12         23         0.425460   \n",
       "\n",
       "        element_2_ratio  element_3_ratio   temp  pressure  air_ratio  \n",
       "0              0.559843         0.000000  283.0     0.775        0.0  \n",
       "1              0.571953         0.000000  303.0     0.800        0.6  \n",
       "2              0.376666         0.280788  283.0     0.500        0.6  \n",
       "3              0.321178         0.625524  293.0     0.900        0.6  \n",
       "4              0.251465         0.277796  273.0     0.500        0.6  \n",
       "...                 ...              ...    ...       ...        ...  \n",
       "183548         0.454117         0.144356  313.0     0.500        0.6  \n",
       "183549         0.389984         0.261341  323.0     1.000        0.6  \n",
       "183550         0.430989         0.452610  313.0     0.100        0.6  \n",
       "183551         0.441890         0.135979  283.0     0.200        0.3  \n",
       "183552         0.037319         0.537220  273.0     0.100        0.6  \n",
       "\n",
       "[183553 rows x 1010 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.concat([pd.read_csv(\"more-elements/more-elements.csv\"), pd.read_csv(\"new-more-data/new-more-data.csv\")])\n",
    "# df\n",
    "df = pd.read_pickle(\"half_data.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183553\n",
      "183553\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df[(df[[str(i) for i in range(1001)]] > 0).all(axis=1)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11451733286843582"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df[\"element_1\"] == 0) | (df[\"element_2\"] == 0) | (df[\"element_3\"] == 0)]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectraDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, device=\"cuda:0\") -> None:\n",
    "        self.data = data\n",
    "        self.elements = self.data[\"element_1\"].unique()\n",
    "        self.air_ratios = self.data.air_ratio.to_numpy(dtype=np.float64)\n",
    "\n",
    "        self.spectras = torch.log(\n",
    "            torch.tensor(df[[str(i) for i in range(1001)]].to_numpy(dtype=np.float64))\n",
    "        ).to(device)\n",
    "\n",
    "        self.ratios = torch.tensor(\n",
    "            df[[\"element_1_ratio\", \"element_2_ratio\", \"element_3_ratio\"]].to_numpy(\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        ).to(device)\n",
    "\n",
    "        self.element_indices = df[[\"element_1\", \"element_2\", \"element_3\"]].to_numpy(\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.elements_distributions = torch.zeros(\n",
    "            [len(self.data), len(self.elements)], dtype=torch.float64\n",
    "        ).to(device)\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            indices = self.element_indices[idx, :]\n",
    "            indices = indices[indices != -1]\n",
    "            self.elements_distributions[idx, indices] = torch.where(self.ratios[idx][\n",
    "                range(indices.shape[0])\n",
    "            ] > 0, 1.0, 0.0).double()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        spectra = self.spectras[idx]\n",
    "        elements_distribution = self.elements_distributions[idx]\n",
    "\n",
    "        return spectra, elements_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzResNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "        )\n",
    "        self.skip_connection = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                in_channels // 4,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                in_channels // 4,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "        self.skip_connection = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1\n",
    "        ) if in_channels != out_channels else lambda x: x\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzCNN(nn.Module):\n",
    "    def __init__(self, n_elements: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_elements = n_elements\n",
    "        self.net = nn.Sequential(\n",
    "            # input_shape: (batch_size, 1, 1001)\n",
    "            nn.BatchNorm1d(1),\n",
    "            THzResNetBlock(in_channels=1, out_channels=64, kernel_size=7),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            # input_shape: (batch_size, 64, 250)\n",
    "            THzResNetBlock(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 128, 50)\n",
    "            THzResNetBlock(in_channels=128, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 256, 10)\n",
    "            THzResNetBlock(in_channels=256, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 512, 2)\n",
    "            nn.Flatten(),\n",
    "            # linear head\n",
    "            nn.Linear(512 * 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.n_elements),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb.wandb_run\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    scheduler: LRScheduler = None,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss = loss_fn(nn.Softmax(dim=1)(pred), target)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    model: nn.Module, dataloader: DataLoader, loss_fn: nn.Module\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for spectra, target in tqdm(dataloader, desc=\"Validating\"):\n",
    "\n",
    "        pred = model(spectra[:, None, :])\n",
    "        loss += float(loss_fn(nn.Softmax(dim=1)(pred), target).item())\n",
    "\n",
    "    return loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: Optimizer,\n",
    "    n_epochs: int,\n",
    "    loss_fn: nn.Module = nn.CrossEntropyLoss(reduction=\"mean\"),\n",
    "    scheduler: LRScheduler = None,\n",
    "    run: wandb.wandb_run.Run = None\n",
    ") -> None:\n",
    "\n",
    "    for i in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, loss_fn, scheduler\n",
    "        )\n",
    "\n",
    "        val_loss = val_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i}: \\n   Train loss: {train_loss:.5f}    |   Val loss: {val_loss:.5f}\\n\"\n",
    "        )\n",
    "        if run:\n",
    "            run.log({\"train_loss\": train_loss, \"val_loss\": val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "n_epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomSpectraDataset(train_df)\n",
    "val_dataset = CustomSpectraDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1fc49446f20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmax23-ost\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\whoee\\Desktop\\CourseWork\\models\\wandb\\run-20240429_020421-vo5z6l5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/max23-ost/course-work/runs/vo5z6l5d' target=\"_blank\">BS=64 detection final</a></strong> to <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/max23-ost/course-work' target=\"_blank\">https://wandb.ai/max23-ost/course-work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/max23-ost/course-work/runs/vo5z6l5d' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/vo5z6l5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"course-work\",\n",
    "    save_code=True,\n",
    "    group=\"CNN\",\n",
    "    name=\"BS=64 detection final\",\n",
    "    notes=\"\",\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = []\n",
    "# for i in range(50000):\n",
    "#     scheduler.step()\n",
    "#     lrs.append(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "THzCNN(\n",
       "  (net): Sequential(\n",
       "    (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (10): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (11): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(64, 256, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (12): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): THzResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (skip_connection): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (14): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (15): THzBottleNeck(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): Dropout(p=0.05, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (post_processing): Sequential(\n",
       "        (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (16): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Flatten(start_dim=1, end_dim=-1)\n",
       "    (18): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (21): ReLU()\n",
       "    (22): Linear(in_features=1024, out_features=25, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = THzCNN(val_dataset[0][1].shape[0])\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "# optimizer = SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler1 = LinearLR(optimizer, start_factor=0.05, end_factor=1, total_iters=500)\n",
    "# scheduler2 = CosineAnnealingWarmRestarts(optimizer, T_0=1500, T_mult=2)\n",
    "# scheduler = SequentialLR(\n",
    "#     optimizer, schedulers=[scheduler1, scheduler2], milestones=[500]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:43<00:00,  3.19it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "   Train loss: 0.20916    |   Val loss: 0.18221\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: \n",
      "   Train loss: 0.16990    |   Val loss: 0.16340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:39<00:00,  3.21it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: \n",
      "   Train loss: 0.16029    |   Val loss: 0.15756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:39<00:00,  3.21it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: \n",
      "   Train loss: 0.15538    |   Val loss: 0.15323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:40<00:00,  3.21it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: \n",
      "   Train loss: 0.15225    |   Val loss: 0.15055\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: \n",
      "   Train loss: 0.14999    |   Val loss: 0.14872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:40<00:00,  3.21it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: \n",
      "   Train loss: 0.14820    |   Val loss: 0.14743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:40<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: \n",
      "   Train loss: 0.14679    |   Val loss: 0.14686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:40<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: \n",
      "   Train loss: 0.14567    |   Val loss: 0.14628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:40<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: \n",
      "   Train loss: 0.14472    |   Val loss: 0.14563\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: \n",
      "   Train loss: 0.14390    |   Val loss: 0.14579\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: \n",
      "   Train loss: 0.14320    |   Val loss: 0.14489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: \n",
      "   Train loss: 0.14260    |   Val loss: 0.14469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: \n",
      "   Train loss: 0.14199    |   Val loss: 0.14408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: \n",
      "   Train loss: 0.14143    |   Val loss: 0.14326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: \n",
      "   Train loss: 0.14097    |   Val loss: 0.14395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: \n",
      "   Train loss: 0.14050    |   Val loss: 0.14349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: \n",
      "   Train loss: 0.13997    |   Val loss: 0.14401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: \n",
      "   Train loss: 0.13949    |   Val loss: 0.14362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: \n",
      "   Train loss: 0.13913    |   Val loss: 0.14357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: \n",
      "   Train loss: 0.13866    |   Val loss: 0.14310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: \n",
      "   Train loss: 0.13822    |   Val loss: 0.14294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: \n",
      "   Train loss: 0.13781    |   Val loss: 0.14287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: \n",
      "   Train loss: 0.13734    |   Val loss: 0.14345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: \n",
      "   Train loss: 0.13691    |   Val loss: 0.14281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: \n",
      "   Train loss: 0.13650    |   Val loss: 0.14276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: \n",
      "   Train loss: 0.13604    |   Val loss: 0.14337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: \n",
      "   Train loss: 0.13561    |   Val loss: 0.14344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: \n",
      "   Train loss: 0.13517    |   Val loss: 0.14314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2438/2438 [12:41<00:00,  3.20it/s]\n",
      "Validating: 100%|██████████| 431/431 [00:46<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: \n",
      "   Train loss: 0.13478    |   Val loss: 0.14379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=run, loss_fn=nn.BCELoss(reduction=\"mean\"))\n",
    "# train(net, train_loader, val_loader, optimizer, n_epochs, scheduler=None, run=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for spectra, target in tqdm(train_loader, desc=\"Training\"):\n",
    "#         optimizer.zero_grad()\n",
    "#         pred = net(spectra[:, None, :])\n",
    "#         loss = nn.CrossEntropyLoss()(pred, target)\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         i += 1\n",
    "#         if i >= 10:\n",
    "#             break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f4f79b0fd5471f8114b033c93585a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.13478</td></tr><tr><td>val_loss</td><td>0.14379</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BS=64 detection final</strong> at: <a href='https://wandb.ai/max23-ost/course-work/runs/vo5z6l5d' target=\"_blank\">https://wandb.ai/max23-ost/course-work/runs/vo5z6l5d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240429_020421-vo5z6l5d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./cnn-detection.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.40 GiB is allocated by PyTorch, and 122.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m preds, y_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m25\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m25\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectra, target \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m----> 3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectra\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((preds, pred), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m     y_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y_val, target), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m, in \u001b[0;36mTHzCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m, in \u001b[0;36mTHzBottleNeck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     54\u001b[0m     unprocessed_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connection(x)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43munprocessed_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.40 GiB is allocated by PyTorch, and 122.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "preds, y_val = torch.empty((0, 25)).to(device), torch.empty((0, 25)).to(device)\n",
    "for spectra, target in val_loader:\n",
    "    pred = net(spectra[:, None, :])\n",
    "    preds = torch.cat((preds, pred), dim=0)\n",
    "    y_val = torch.cat((y_val, target), dim=0)\n",
    "preds.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
