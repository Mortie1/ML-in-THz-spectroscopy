{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_USE_CUDA_DSA=1\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_USE_CUDA_DSA=1\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch.optim.lr_scheduler import LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1000</th>\n",
       "      <th>element_1</th>\n",
       "      <th>element_2</th>\n",
       "      <th>element_3</th>\n",
       "      <th>element_1_ratio</th>\n",
       "      <th>element_2_ratio</th>\n",
       "      <th>element_3_ratio</th>\n",
       "      <th>temp</th>\n",
       "      <th>pressure</th>\n",
       "      <th>air_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51976</th>\n",
       "      <td>1.429725e-20</td>\n",
       "      <td>3.325318e-20</td>\n",
       "      <td>1.513897e-20</td>\n",
       "      <td>1.119836e-21</td>\n",
       "      <td>1.209848e-21</td>\n",
       "      <td>2.024112e-21</td>\n",
       "      <td>7.172782e-21</td>\n",
       "      <td>4.953834e-21</td>\n",
       "      <td>1.539859e-21</td>\n",
       "      <td>1.285451e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>9.105786e-25</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0.136030</td>\n",
       "      <td>0.262144</td>\n",
       "      <td>0.601825</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18970</th>\n",
       "      <td>5.940243e-23</td>\n",
       "      <td>7.336601e-23</td>\n",
       "      <td>9.720283e-23</td>\n",
       "      <td>1.447083e-22</td>\n",
       "      <td>2.680262e-22</td>\n",
       "      <td>8.343595e-22</td>\n",
       "      <td>1.010416e-20</td>\n",
       "      <td>6.042984e-21</td>\n",
       "      <td>7.479574e-22</td>\n",
       "      <td>3.643073e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.465311e-22</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>0.188355</td>\n",
       "      <td>0.316809</td>\n",
       "      <td>0.494835</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30497</th>\n",
       "      <td>1.193615e-20</td>\n",
       "      <td>6.945339e-21</td>\n",
       "      <td>6.543001e-22</td>\n",
       "      <td>2.529143e-22</td>\n",
       "      <td>1.749403e-22</td>\n",
       "      <td>3.243140e-22</td>\n",
       "      <td>3.550860e-22</td>\n",
       "      <td>1.091960e-22</td>\n",
       "      <td>9.629268e-23</td>\n",
       "      <td>1.882503e-22</td>\n",
       "      <td>...</td>\n",
       "      <td>7.445923e-23</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>0.444071</td>\n",
       "      <td>0.477905</td>\n",
       "      <td>0.078024</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30356</th>\n",
       "      <td>1.624151e-20</td>\n",
       "      <td>5.577240e-21</td>\n",
       "      <td>9.267746e-22</td>\n",
       "      <td>2.168774e-21</td>\n",
       "      <td>5.549700e-21</td>\n",
       "      <td>4.163047e-21</td>\n",
       "      <td>1.670818e-21</td>\n",
       "      <td>8.912111e-22</td>\n",
       "      <td>8.374945e-22</td>\n",
       "      <td>1.444200e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>3.604447e-22</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>0.509421</td>\n",
       "      <td>0.224513</td>\n",
       "      <td>0.266066</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140548</th>\n",
       "      <td>2.003258e-21</td>\n",
       "      <td>1.077312e-21</td>\n",
       "      <td>3.272797e-22</td>\n",
       "      <td>1.452913e-22</td>\n",
       "      <td>9.167639e-23</td>\n",
       "      <td>6.963908e-23</td>\n",
       "      <td>5.832702e-23</td>\n",
       "      <td>5.251748e-23</td>\n",
       "      <td>4.962818e-23</td>\n",
       "      <td>4.851140e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>9.615524e-22</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0.227958</td>\n",
       "      <td>0.482563</td>\n",
       "      <td>0.289478</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218031</th>\n",
       "      <td>3.535618e-22</td>\n",
       "      <td>4.722553e-22</td>\n",
       "      <td>4.692500e-22</td>\n",
       "      <td>5.307244e-22</td>\n",
       "      <td>6.048017e-22</td>\n",
       "      <td>6.291295e-22</td>\n",
       "      <td>6.453254e-22</td>\n",
       "      <td>6.485575e-22</td>\n",
       "      <td>7.700791e-22</td>\n",
       "      <td>1.061828e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>8.640137e-25</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.530767</td>\n",
       "      <td>0.469233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57587</th>\n",
       "      <td>9.397569e-22</td>\n",
       "      <td>1.173034e-21</td>\n",
       "      <td>2.279960e-21</td>\n",
       "      <td>8.301202e-22</td>\n",
       "      <td>8.418463e-22</td>\n",
       "      <td>9.303921e-22</td>\n",
       "      <td>1.093095e-21</td>\n",
       "      <td>2.257390e-21</td>\n",
       "      <td>3.849821e-21</td>\n",
       "      <td>3.221007e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020158e-21</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>0.600135</td>\n",
       "      <td>0.375171</td>\n",
       "      <td>0.024694</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254241</th>\n",
       "      <td>7.204798e-22</td>\n",
       "      <td>1.141918e-21</td>\n",
       "      <td>2.021069e-21</td>\n",
       "      <td>3.898292e-21</td>\n",
       "      <td>4.167931e-21</td>\n",
       "      <td>3.322392e-21</td>\n",
       "      <td>2.260052e-21</td>\n",
       "      <td>1.356805e-21</td>\n",
       "      <td>9.869838e-22</td>\n",
       "      <td>1.119172e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174975e-22</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>0.273032</td>\n",
       "      <td>0.469203</td>\n",
       "      <td>0.257765</td>\n",
       "      <td>293.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30894</th>\n",
       "      <td>4.030195e-22</td>\n",
       "      <td>5.592868e-22</td>\n",
       "      <td>9.234086e-22</td>\n",
       "      <td>2.112966e-21</td>\n",
       "      <td>5.189306e-21</td>\n",
       "      <td>3.964872e-21</td>\n",
       "      <td>1.755474e-21</td>\n",
       "      <td>1.090197e-21</td>\n",
       "      <td>1.088081e-21</td>\n",
       "      <td>1.695364e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.690917e-23</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>0.375711</td>\n",
       "      <td>0.384662</td>\n",
       "      <td>0.239627</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32552</th>\n",
       "      <td>5.608307e-21</td>\n",
       "      <td>5.375661e-21</td>\n",
       "      <td>3.065098e-21</td>\n",
       "      <td>2.121334e-20</td>\n",
       "      <td>1.853282e-20</td>\n",
       "      <td>1.564238e-21</td>\n",
       "      <td>8.492103e-22</td>\n",
       "      <td>1.319094e-21</td>\n",
       "      <td>6.949287e-22</td>\n",
       "      <td>1.153705e-21</td>\n",
       "      <td>...</td>\n",
       "      <td>7.678208e-24</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.296077</td>\n",
       "      <td>0.303199</td>\n",
       "      <td>0.400723</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131079 rows × 1010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0             1             2             3             4  \\\n",
       "51976   1.429725e-20  3.325318e-20  1.513897e-20  1.119836e-21  1.209848e-21   \n",
       "18970   5.940243e-23  7.336601e-23  9.720283e-23  1.447083e-22  2.680262e-22   \n",
       "30497   1.193615e-20  6.945339e-21  6.543001e-22  2.529143e-22  1.749403e-22   \n",
       "30356   1.624151e-20  5.577240e-21  9.267746e-22  2.168774e-21  5.549700e-21   \n",
       "140548  2.003258e-21  1.077312e-21  3.272797e-22  1.452913e-22  9.167639e-23   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "218031  3.535618e-22  4.722553e-22  4.692500e-22  5.307244e-22  6.048017e-22   \n",
       "57587   9.397569e-22  1.173034e-21  2.279960e-21  8.301202e-22  8.418463e-22   \n",
       "254241  7.204798e-22  1.141918e-21  2.021069e-21  3.898292e-21  4.167931e-21   \n",
       "30894   4.030195e-22  5.592868e-22  9.234086e-22  2.112966e-21  5.189306e-21   \n",
       "32552   5.608307e-21  5.375661e-21  3.065098e-21  2.121334e-20  1.853282e-20   \n",
       "\n",
       "                   5             6             7             8             9  \\\n",
       "51976   2.024112e-21  7.172782e-21  4.953834e-21  1.539859e-21  1.285451e-21   \n",
       "18970   8.343595e-22  1.010416e-20  6.042984e-21  7.479574e-22  3.643073e-22   \n",
       "30497   3.243140e-22  3.550860e-22  1.091960e-22  9.629268e-23  1.882503e-22   \n",
       "30356   4.163047e-21  1.670818e-21  8.912111e-22  8.374945e-22  1.444200e-21   \n",
       "140548  6.963908e-23  5.832702e-23  5.251748e-23  4.962818e-23  4.851140e-23   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "218031  6.291295e-22  6.453254e-22  6.485575e-22  7.700791e-22  1.061828e-21   \n",
       "57587   9.303921e-22  1.093095e-21  2.257390e-21  3.849821e-21  3.221007e-21   \n",
       "254241  3.322392e-21  2.260052e-21  1.356805e-21  9.869838e-22  1.119172e-21   \n",
       "30894   3.964872e-21  1.755474e-21  1.090197e-21  1.088081e-21  1.695364e-21   \n",
       "32552   1.564238e-21  8.492103e-22  1.319094e-21  6.949287e-22  1.153705e-21   \n",
       "\n",
       "        ...          1000  element_1  element_2  element_3  element_1_ratio  \\\n",
       "51976   ...  9.105786e-25          9         12         22         0.136030   \n",
       "18970   ...  1.465311e-22          0         21         22         0.188355   \n",
       "30497   ...  7.445923e-23          5         16         19         0.444071   \n",
       "30356   ...  3.604447e-22          4         21         24         0.509421   \n",
       "140548  ...  9.615524e-22          4         13         18         0.227958   \n",
       "...     ...           ...        ...        ...        ...              ...   \n",
       "218031  ...  8.640137e-25          6          9         -1         0.530767   \n",
       "57587   ...  1.020158e-21         14         15         18         0.600135   \n",
       "254241  ...  1.174975e-22         10         18         20         0.273032   \n",
       "30894   ...  1.690917e-23          8         14         24         0.375711   \n",
       "32552   ...  7.678208e-24          2         16         20         0.296077   \n",
       "\n",
       "        element_2_ratio  element_3_ratio   temp  pressure  air_ratio  \n",
       "51976          0.262144         0.601825  283.0       0.2        0.6  \n",
       "18970          0.316809         0.494835  283.0       0.1        0.6  \n",
       "30497          0.477905         0.078024  323.0       0.1        0.6  \n",
       "30356          0.224513         0.266066  323.0       0.1        0.6  \n",
       "140548         0.482563         0.289478  263.0       0.5        0.6  \n",
       "...                 ...              ...    ...       ...        ...  \n",
       "218031         0.469233         0.000000  323.0       0.8        0.6  \n",
       "57587          0.375171         0.024694  303.0       0.2        0.6  \n",
       "254241         0.469203         0.257765  293.0       1.0        0.6  \n",
       "30894          0.384662         0.239627  323.0       0.1        0.6  \n",
       "32552          0.303199         0.400723  253.0       0.2        0.3  \n",
       "\n",
       "[131079 rows x 1010 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle(\"test_proc.pkl\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectraDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, device=\"cuda:0\") -> None:\n",
    "        self.data = data\n",
    "        self.elements = self.data[\"element_1\"].unique()\n",
    "        # self.air_ratios = data.air_ratio.to_numpy(dtype=np.float64)\n",
    "\n",
    "        self.spectras = torch.log(\n",
    "            torch.tensor(\n",
    "                self.data[[str(i) for i in range(1001)]].to_numpy(dtype=np.float64)\n",
    "            )\n",
    "        ).to(device)\n",
    "\n",
    "        self.ratios = torch.tensor(\n",
    "            self.data[\n",
    "                [\"element_1_ratio\", \"element_2_ratio\", \"element_3_ratio\"]\n",
    "            ].to_numpy(dtype=np.float64)\n",
    "        ).to(device)\n",
    "\n",
    "        self.element_indices = self.data[\n",
    "            [\"element_1\", \"element_2\", \"element_3\"]\n",
    "        ].to_numpy(dtype=np.float64)\n",
    "\n",
    "        self.elements_distributions = torch.zeros(\n",
    "            [len(self.data), len(self.elements)], dtype=torch.float64\n",
    "        ).to(device)\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            indices = self.element_indices[idx, :]\n",
    "            indices = indices[indices != -1]\n",
    "            self.elements_distributions[idx, indices] = torch.where(self.ratios[idx][\n",
    "                range(indices.shape[0])\n",
    "            ] > 0, 1.0, 0.0).double()\n",
    "\n",
    "        self.elements_distributions = self.elements_distributions[\n",
    "            ~torch.isnan(self.spectras).any(dim=1)\n",
    "        ]\n",
    "        self.spectras = self.spectras[~torch.isnan(self.spectras).any(dim=1)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.spectras)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        spectra = self.spectras[idx]\n",
    "        elements_distribution = self.elements_distributions[idx]\n",
    "\n",
    "        return spectra, elements_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomSpectraDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzResNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "        )\n",
    "        self.skip_connection = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.05,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels,\n",
    "                in_channels // 4,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                in_channels // 4,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.Dropout(p=dropout),\n",
    "            activation,\n",
    "            nn.Conv1d(\n",
    "                in_channels // 4,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "            ),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "        self.skip_connection = (\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else lambda x: x\n",
    "        )\n",
    "        self.post_processing = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.out_channels),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        unprocessed_result = self.layers(x) + self.skip_connection(x)\n",
    "        return self.post_processing(unprocessed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class THzCNN(nn.Module):\n",
    "    def __init__(self, n_elements: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_elements = n_elements\n",
    "        self.net = nn.Sequential(\n",
    "            # input_shape: (batch_size, 1, 1001)\n",
    "            nn.BatchNorm1d(1),\n",
    "            THzResNetBlock(in_channels=1, out_channels=64, kernel_size=7),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            # input_shape: (batch_size, 64, 250)\n",
    "            THzResNetBlock(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 128, 50)\n",
    "            THzResNetBlock(in_channels=128, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 256, 10)\n",
    "            THzResNetBlock(in_channels=256, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            THzBottleNeck(in_channels=512, out_channels=512, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size=5),\n",
    "            # input_shape: (batch_size, 512, 2)\n",
    "            nn.Flatten(),\n",
    "            # linear head\n",
    "            nn.Linear(512 * 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.n_elements),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing:   0%|          | 0/2048 [00:00<?, ?it/s]C:\\Users\\whoee\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "testing: 100%|██████████| 2048/2048 [04:09<00:00,  8.20it/s]\n"
     ]
    }
   ],
   "source": [
    "net = THzCNN(test_dataset[0][1].shape[0])\n",
    "net.load_state_dict(torch.load(\"./cnn-detection.model\"))\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "pred, y_test = np.empty((0, 25)), np.empty((0, 25))\n",
    "for spectra, target in tqdm(val_loader, desc=\"testing\"):\n",
    "    net.eval()\n",
    "    ans = net(spectra[:, None, :])\n",
    "    ans = nn.Softmax()(ans).cpu().detach().numpy()\n",
    "    pred = np.append(pred, ans, axis=0)\n",
    "    y_test = np.append(y_test, target.cpu().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing:   0%|          | 0/4095 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1001 elements not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectra, target \u001b[38;5;129;01min\u001b[39;00m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     ans \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax()(ans)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m, in \u001b[0;36mTHzCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 1001 elements not 1"
     ]
    }
   ],
   "source": [
    "# for spectra, target in tqdm(val_loader, desc=\"testing\"):\n",
    "#     ans = net(spectra)\n",
    "#     ans = nn.Softmax()(ans)\n",
    "#     break\n",
    "# ans[5], target[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions.\n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1.0 - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets * np.log(predictions + 1e-9)) / N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.019411249832876582\n",
      "MAE: [0.00640533 0.02327715 0.02247833 0.00866646 0.00921007 0.00726134\n",
      " 0.01036512 0.00942259 0.02407267 0.01124802 0.00970918 0.0109634\n",
      " 0.00743144 0.0093933  0.00862645 0.01011655 0.00979261 0.00836744\n",
      " 0.00904559 0.00907102 0.00802924 0.00961578 0.011302   0.01069435\n",
      " 0.01134808 0.22877898]\n",
      "Cross Entropy: 2.105780903335893\n"
     ]
    }
   ],
   "source": [
    "# reg\n",
    "print(f'MAE: {mean_absolute_error(y_test, pred)}')\n",
    "print(f\"MAE: {mean_absolute_error(y_test, pred, multioutput='raw_values')}\")\n",
    "print(f\"Cross Entropy: {cross_entropy(y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9402385036137177\n",
      "precision: 0.9390588682468852\n",
      "recall: 0.9425612194580725\n",
      "accuracy: 0.7568512229556988\n"
     ]
    }
   ],
   "source": [
    "# detection\n",
    "print(f'F1: {f1_score(y_test, np.where(pred > 0.12, 1, 0), average=\"macro\")}')\n",
    "print(\n",
    "    f'precision: {precision_score(y_test, np.where(pred > 0.12, 1, 0), average=\"macro\")}'\n",
    ")\n",
    "print(f'recall: {recall_score(y_test, np.where(pred > 0.12, 1, 0), average=\"macro\")}')\n",
    "print(f\"accuracy: {accuracy_score(y_test, np.where(pred > 0.12, 1, 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [1.         0.52313939 0.63120567 1.         0.99993346 0.99740329\n",
      " 0.89652628 0.99871352 0.46923328 0.99657603 0.99986556 0.9995314\n",
      " 0.99993337 0.99993265 0.99320996 0.99973378 0.9989235  0.99966808\n",
      " 0.9997348  1.         0.99886531 1.         0.99152827 0.99953321\n",
      " 0.98328088]\n",
      "recall: [0.99834831 0.63031968 0.55364548 0.99292909 0.99675002 0.99727049\n",
      " 0.96883928 0.97507768 0.57104042 0.97657895 0.98778139 0.99130262\n",
      " 0.9978722  0.99550758 0.99699298 0.99774161 0.99657672 0.99787953\n",
      " 0.9963658  0.99660181 0.99073155 0.99118119 0.98556439 0.99409736\n",
      " 0.98703436]\n",
      "F1: [0.99917347 0.5717499  0.58988704 0.996452   0.9983392  0.99733688\n",
      " 0.93128113 0.98675408 0.51515514 0.98647616 0.99378675 0.9954\n",
      " 0.99890172 0.99771521 0.99509788 0.9987367  0.99774873 0.99877301\n",
      " 0.99804746 0.99829801 0.9947818  0.99557106 0.98853734 0.99680787\n",
      " 0.98515404]\n"
     ]
    }
   ],
   "source": [
    "print(f\"precision: {precision_score(y_test, np.where(pred > 0.12, 1, 0), average=None)}\")\n",
    "print(f\"recall: {recall_score(y_test, np.where(pred > 0.12, 1, 0), average=None)}\")\n",
    "print(f\"F1: {f1_score(y_test, np.where(pred > 0.12, 1, 0), average=None)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
